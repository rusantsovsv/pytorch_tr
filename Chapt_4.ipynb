{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование упреждающих сетей при NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определим глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [0, 0, 1, 1] # обозначение множеств\n",
    "CENTERS = [(-3, -3), (3, 3), (3, -3), (-3, 3)] # центры множеств"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=2, output_size=3,\n",
    "                 num_hidden_layers=1, hidden_activation=nn.Sigmoid):\n",
    "        \"\"\"\n",
    "        Инициализируем веса.\n",
    "        Аргументы:\n",
    "            input_dim (int): размер входных векторов\n",
    "            hidden_dim (int): размер выходных результатов первого линейного слоя\n",
    "            output_dim (int): размер выходных результатов второго линейного слоя \n",
    "            num_hidden_layers(int): количество скрытых слоев\n",
    "            hidden_activation(torch.nn.*): функция активации\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        # модуль, содержащий список слоев\n",
    "        self.module_list = nn.ModuleList()\n",
    "        \n",
    "        interim_input_size = input_size\n",
    "        interim_output_size = hidden_size\n",
    "        \n",
    "        # создаем скрытые слои\n",
    "        for _ in range(num_hidden_layers):\n",
    "            # добавляем слой, учитывая его размер\n",
    "            self.module_list.append(nn.Linear(interim_input_size, interim_output_size))\n",
    "            # добавляем функцию активации\n",
    "            self.module_list.append(hidden_activation())\n",
    "            interim_input_size = interim_output_size\n",
    "            \n",
    "        self.fc_final = nn.Linear(interim_input_size, output_size)\n",
    "        self.last_forward_cache = []\n",
    "        \n",
    "    def forward(self, x, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        Прямой проход MLP\n",
    "        Аргументы:\n",
    "            x_in (torch.Tensor): тензор входных данных\n",
    "            Значение x_in.shape должно быть (batch, input_dim)\n",
    "            apply_softmax (bool): флаг для многомерной логистической функции\n",
    "            активации. При использовании функции потерь на основе\n",
    "            перекрестной энтропии должен равняться false\n",
    "        Возвращает:\n",
    "            итоговый тензор. Значение tensor.shape должно\n",
    "            быть (batch, output_dim)\n",
    "        \"\"\"\n",
    "        self.last_forward_cache = []\n",
    "        self.last_forward_cache.append(x.to('cpu').numpy())\n",
    "        \n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "            self.last_forward_cache.append(x.to('cpu').data.numpy)\n",
    "            \n",
    "        output = self.fc_final(x)\n",
    "        self.last_forward_cache.append(output.to('cpu').data.numpy())\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сгенерируем данные для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_data(batch_size):\n",
    "    assert len(CENTERS) == len(LABELS), 'количество центров должно быть равно количеству меток'\n",
    "    \n",
    "    x_data = []\n",
    "    y_targets = np.zeros(batch_size)\n",
    "    n_centers = len(CENTERS)\n",
    "    \n",
    "    for batch_i in range(batch_size):\n",
    "        center_idx = np.random.randint(0, n_centers)\n",
    "        x_data.append(np.random.normal(loc=CENTERS[center_idx]))\n",
    "        y_targets[batch_i] = LABELS[centr_idx]\n",
    "        \n",
    "    return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_targets, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(perceptron, x_data, y_truth, n_samples=1000, ax=None, epoch=None,\n",
    "                      title='', levels=[0.3, 0.4, 0.5], linestyles=['--', '-', '--']):\n",
    "    _, y_pred = perceptron(x_data, apply_softmax=True).max(dim=1)\n",
    "    y_pred = y_pred.data.numpy()\n",
    "    \n",
    "    x_data = x_data.data.numpy()\n",
    "    y_truth = y_truth.data.numpy()\n",
    "    \n",
    "    n_classes = len(set(LABELS))\n",
    "    \n",
    "    all_x = [[] for _ in range(n_classes)]\n",
    "    all_colors = [[] for _ in range(n_classes)]\n",
    "    \n",
    "    colors = ['black', 'white']\n",
    "    markers = ['o', '*']\n",
    "    \n",
    "    for x_i, y_pred_i, y_true_i in zip(x_data, y_pred, y_truth):\n",
    "        all_x[y_true_i].append(x_i)\n",
    "        if y_pred_i == y_true_i:\n",
    "            all_colors[y_true_i].append('white')\n",
    "        else:\n",
    "            all_colors[y_true_i].append('black')\n",
    "        # all_colors[y_true_i].append(colors[y_pred_i])\n",
    "        \n",
    "    all_x = [np.stack(x_list) for x_list in all_x]\n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        \n",
    "    for x_list, color_list, marker in zip(all_x, all_colors, markers):\n",
    "        ax.scatter(x_list[:, 0], x_list[:, 1], edgecolor='black', marker=marker, facecolor=color_list, s=100)\n",
    "        \n",
    "    xlim = (min([x_list[:, 0].min() for x_list in all_x])), (max([x_list[:, 0].min() for x_list in all_x]))\n",
    "    \n",
    "    ylim = (min([x_list[:, 1].min() for x_list in all_x])), (max([x_list[:, 1].min() for x_list in all_x]))\n",
    "    \n",
    "    # строим гиперплоскость\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linstace(ylim[0], ylim[1], 30)    \n",
    "    YY, XX = np.meshgrid(yy, xx)    \n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        Z = perceptron(torch.tensor(xy, dtype=torch.float32), apply_softmax=True)\n",
    "        Z = Z[:, i].data.numpy().reshape(XX.shape)\n",
    "        ax.contour(XX, YY, Z, colors=colors[i], levels=levels, linestyles=linestyles)\n",
    "        \n",
    "    plt.suptitle(title)\n",
    "    \n",
    "    if epoch is not None:\n",
    "        plt.text(xlim[0], ylim[0], 'Epoch = {}'.format(str(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
