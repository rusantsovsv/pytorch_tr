{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая фаза преобразования текста в векторизованный мини-пакет — отображение токенов в числовую форму. Стандартный его вариант — взаимно однозначное, то есть обратимое отображение — между токенами и числами. На языке Python они будут представлять собой два словаря. Мы инкапсулируем это взаимно однозначное соответствие в классе Vocabulary.\n",
    "\n",
    "\n",
    "Класс Vocabulary не только отвечает за это взаимно однозначное соответствие, благодаря которому пользователь может добавлять новые токены и автоматически наращивать значение индекса, но и поддерживает специальный токен UNK, название которого расшифровывается как «неизвестный» (unknown). Благодаря UNK можно при контроле обрабатывать токены, которые алгоритм не видел при обучении (например, при контроле могут встретиться слова, которых не было в обучающем наборе данных). Как вы увидите в следующем пункте, мы даже будем явным образом исключать нечасто встречающиеся токены из Vocabulary, так что в процедуре обучения будут встречаться токены UNK. Они играют важную роль в уменьшении объема используемой классом Vocabulary памяти. Ожидается, что для добавления новых токенов в Vocabulary будет вызываться метод add_token(), для извлечения индекса токена — метод lookup_token() и для извлечения соответствующего конкретному индексу токена — lookup_index()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Класс для обработки текста и извлечения словарного запаса\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "        token_to_idx(dict): готовый ассоциативный массив соответствий токенов индексам \n",
    "        add_unk (bool): флаг, указывающий, нужно ли добавлять токен UNK\n",
    "        unk_token (str): добавляемый в словарь токен UNK\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = 1\n",
    "        \n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializable(self):\n",
    "        \"\"\"Возвращает словарь с возможностью сериализации(сохранения в файл)\"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Создает экземпляр Vocabulary на основе сериализованного словаря\"\"\"\n",
    "        return cls(**contents)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Обновляет словари отображения, добавляя в них токен.\n",
    "            Аргументы:\n",
    "            token (str): добавляемый в Vocabulary элемент\n",
    "            Возвращает:\n",
    "            index (int): соответствующее токену целочисленное значение\"\"\"\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Извлекает соответствующий токену индекс\n",
    "        или индекс UNK, если токен не найден.\n",
    "        Аргументы:\n",
    "        token (str): токен для поиска\n",
    "        Возвращает:\n",
    "        index (int): соответствующий токену индекс\n",
    "        Примечания:\n",
    "        'unk_index' должен быть >=0 (добавлено в Vocabulary)\n",
    "        для должного функционирования UNK\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" Возвращает соответствующий индексу токен\n",
    "        Аргументы:\n",
    "        index (int): индекс для поиска\n",
    "        Возвращает:\n",
    "        token (str): соответствующий индексу токен\n",
    "        Генерирует:\n",
    "        KeyError: если индекс не найден в Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError('the index (%d) is not in the Vocabulary' % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '<Vocabulary(size=%d)>' % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс ReviewVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая фаза преобразования текста в векторизованный мини-пакет — проход в цикле по токенам входных данных и преобразование каждого из них в цифровую форму. В результате этого должен получиться вектор. А поскольку этот вектор затем объединяется с векторами для других точек данных, возникает ограничение, гласящее, что длина создаваемых классом Vectorizer векторов должна быть одинаковой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\"Векторизатор, приводящий длину векторов корпусов к единому размеру\"\"\"\n",
    "    \n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"Аргументы:\n",
    "                    review_vocab (Vocabulary): отображает слова\n",
    "                    в целочисленные значения\n",
    "                    \n",
    "                    rating_vocab (Vocabulary): отображает метки классов\n",
    "                    в целочисленные значения\"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "        \n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Создает свернутый унитарный вектор для обзора\n",
    "        Аргументы:\n",
    "        review (str): обзор\n",
    "        Возвращает:\n",
    "        one_hot (np.ndarray): свернутое унитарное представление\n",
    "        \"\"\"\n",
    "        \n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for token in review.split(' '):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "                \n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\" Создает экземпляр векторизатора на основе\n",
    "        объекта DataFrame набора данных\n",
    "        Аргументы:\n",
    "        review_df (pandas.DataFrame): набор данных обзоров\n",
    "        cutoff (int): параметр для фильтрации по частоте вхождения\n",
    "        Возвращает:\n",
    "        экземпляр класса ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # добавляем рейтинг\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        # Добавляем часто используемые слова, если число вхождений больше указанного\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "                    \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Создает экземпляр ReviewVectorizer на основе\n",
    "        сериализуемого словаря\n",
    "        Аргументы:\n",
    "        contents (dict): сериализуемый словарь\n",
    "        Возвращает:\n",
    "        экземпляр класса ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        \n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Создает сериализуемый словарь для кэширования\n",
    "        Возвращает:\n",
    "        contents (dict): сериализуемый словарь\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader(Класс ReviewDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя фаза конвейера преобразования текста в векторизованный мини-пакет — собственно группировка векторизованных точек данных. Поскольку группировка в мини-пакеты играет столь важную роль в обучении нейронных сетей, фреймворк PyTorch предоставляет для координации этого процесса встроенный класс DataLoader. Для создания экземпляра класса DataLoader необходимо передать какой-либо объект Dataset PyTorch (например, описанный нами для этого примера ReviewDataset), batch_size и несколько других поименованных аргументов. В результате получается объект, представляющий собой Python-итератор, группирующий и свертывающий содержащиеся в объекте Dataset точки данных/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            review_df (pandas.DataFrame): датасет\n",
    "            vectorizer (ReviewVectorizer): векторизованное представление датасета\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size),}\n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"\n",
    "        Загрузить набор данных и создать векторизированную форму с нуля\n",
    "        Аргументы:\n",
    "        review_csv (str): расположение датасета\n",
    "        Returns:\n",
    "            экземпляр ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        '''\n",
    "        Загрузка датасета и соответствующего векторизатора\n",
    "        Используется в том случае, когда векторизатор был кэширован для повторного использования\n",
    "        \n",
    "        Аргументы:\n",
    "        vectorizer_filepath (str): расположение векторизатора\n",
    "        Returns:\n",
    "            экземпляр ReviewVectorizer\n",
    "        '''\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        '''\n",
    "        Статический метод для только загрузки векторизатора из файла\n",
    "        '''\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        '''Сохраняет векторизатор в файл json'''\n",
    "        with open(vectorizer_filepath, 'w') as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        '''Возвратить векторизатор'''\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        '''Выбрать столбец для разбиения датасета на подвыборки\n",
    "        аргументы:\n",
    "            split(str): один из 'train', 'val', 'test'\n",
    "        '''\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "        \"\"\"для наследования класса Dataset PyTorch разработчик должен реализовать методы __getitem__() и __len__(), \n",
    "        благодаря чему класс DataLoader сможет пройти в цикле по набору данных путем итерации по индексам из этого набора.\"\"\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Первичная точка входа для датасетов Pytorch\n",
    "        args:\n",
    "            index(int): индекс для доступа к значениям данным\n",
    "        Return:\n",
    "            словарь, содержащий значения данных (x_data) и метку (y_target)\n",
    "        '''\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "        \n",
    "        return {'x_data': review_vector, \n",
    "                'y_target': rating_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        '''Возвращает количество батчей в датасете'''\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим для DataLoader адаптер в виде функции generate_batches() — генератора для удобного выбора (switch) данных между CPU и GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device='cpu'):\n",
    "    '''\n",
    "    Функция-генератор — адаптер для объекта DataLoader фреймворка PyTorch.\n",
    "    Гарантирует размещение всех тензоров на нужном устройстве.\n",
    "    '''\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификатор-перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемая в этом примере модель представляет собой новую реализацию классификатора Perceptron. Класс ReviewClassifier наследует класс Module фреймворка PyTorch и создает слой преобразования типа Linear, возвращающего один результат. Поскольку речь идет о бинарной классификации (обзор может быть позитивным или негативным), этого вполне достаточно. В качестве завершающего нелинейного преобразования используется сигма-функция.\n",
    "\n",
    "Благодаря параметризации метода forward() применение сигма-функции необязательно. Чтобы разобраться, для чего это нужно, укажем сначала, что наиболее подходящей функцией потерь для задачи бинарной классификации является бинарная функция потерь на основе перекрестной энтропии (torch.nn.BCELoss()). Она специально сформулирована математически в расчете на бинарные вероятности. Однако в случае применения сигма-функции, а затем этой функции потерь возникают проблемы с численной устойчивостью. PyTorch предоставляет пользователям более устойчивый численно вариант — BCEWithLogitsLoss(). При использовании функции потерь не следует применять сигма-функцию (поэтому по умолчанию она у нас не применяется). Но если пользователю классификатора хотелось бы получить вероятностное значение, понадобится сигма-функция, так что такую возможность необходимо оставить в качестве необязательной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    '''Классификатор на основе перцептрона'''\n",
    "    def __init__(self, num_features):\n",
    "        '''\n",
    "        num_features: размер входного вектора признаков\n",
    "        '''\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        '''Прямой проход классификатора\n",
    "        Аргументы:\n",
    "            x_in (torch.Tensor): входной тензор данных\n",
    "            x_in.shape должен быть (batch, num_features)\n",
    "            apply_sigmoid (bool): флаг для сигма-функции активации\n",
    "            при использовании функции потерь на основе перекрестной\n",
    "            энтропии должен равняться false\n",
    "        Возращает:\n",
    "            итоговый тензор. tensor.shape должен быть (batch,).\n",
    "        '''\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Процедура обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом подразделе мы в общих чертах опишем компоненты процедуры обучения и их взаимодействие с набором данных и моделью с целью подбора параметров модели и повышения ее эффективности. По сути, процедура обучения отвечает за воплощение модели, проход в цикле по набору данных, вычисление (на основе переданных входных данных) выходных значений модели, определение потерь (того, насколько модель ошиблась в предсказаниях) и обновление модели в соответствии с потерями. Хотя на первый взгляд здесь есть масса нюансов, на самом деле точек модификации процедуры обучения не так уж много, поэтому вы скоро привыкнете к ней в процессе разработки алгоритмов глубокого обучения.\n",
    "\n",
    "Для упрощения управления высокоуровневыми решениями мы сосредоточим в объекте args все точки принятия решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # информация о данных и путях\n",
    "    frequency_cutoff=25, \n",
    "    model_state_file='model.pth', \n",
    "    review_csv='data/yelp/reviews_with_splits_full.csv',\n",
    "    save_dir='model_storage/ch3/yelp',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # нет гиперпараметров модели\n",
    "    # есть гиперпараметры обучения\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=5,\n",
    "    seed=1137,\n",
    "    # параметры запуска\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зададим состояние обучения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    \n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка обновлений состояний обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"\n",
    "    Составные части:\n",
    "      - Ранняя остановка: предотвратить переобучение.\n",
    "      - Контрольная точка модели: модель сохраняется, если модель лучше\n",
    "    Аргументы:\n",
    "     - param args: основные аргументы\n",
    "     - парам модель: модель к обучению\n",
    "     - param train_state: словарь, представляющий значения состояния обучения\n",
    "     - возвращает:\n",
    "         новый train_state\n",
    "    \"\"\"\n",
    "    \n",
    "    # сохранить хотябы одну модель\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "        \n",
    "    # сохранить, если модель улучшилась\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "        \n",
    "        # если Loss ухудшился(увеличился)\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # обновить шаг\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # loss уменьшился\n",
    "        else:\n",
    "            # нухно сохранить лучшую модель\n",
    "            torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            # сбросим шаг ранней остановки\n",
    "            train_state['early_stopping_step'] = 0\n",
    "            \n",
    "        # остановить ранее?\n",
    "        train_state['stop_early'] = train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    \n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расчет accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще полезные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация генератора случайных чисел\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# проверка на существование директорий\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расширенные пути к файлам: \n",
      "\tmodel_storage/ch3/yelp/vectorizer.json\n",
      "\tmodel_storage/ch3/yelp/model.pth\n"
     ]
    }
   ],
   "source": [
    "# Добавим пути к файлам\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = args.save_dir + '/' + args.vectorizer_file\n",
    "    args.model_state_file = args.save_dir + '/' + args.model_state_file\n",
    "    print('Расширенные пути к файлам: ')\n",
    "    print('\\t{}'.format(args.vectorizer_file))\n",
    "    print('\\t{}'.format(args.model_state_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим доступность CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    print('Not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Использование CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Использование CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зададим параметры воспроизводимости результатов\n",
    "set_seed_everywhere(args.seed, args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим и создадим директорию для сохранения файлов\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Готовим датасет**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подгружаем датасет и создаем векторизатор\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # из сохраненного\n",
    "    print('Подгружаем датасет и векторизатор')\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv, args.vectorizer_file)\n",
    "else:\n",
    "    print('Подгружаем датасет и создаем векторизатор')\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# создадим объект классификатора\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device) # зададим параметры классификатора в зависимости от способа обучения - на CPU или на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rusancovs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6837c5b64954bc2a3cc28861587e589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', max=5.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rusancovs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b5b7a703b64419990060f8d5b3ba63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=3062.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rusancovs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:33: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42901fc6385b455cb5ad6ad42ccddfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=1312.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting loop\n"
     ]
    }
   ],
   "source": [
    "# выберем Loss функцию BCE с логистическими потерями\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# воспользуемся оптимизатором Adam\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# определим планировщик:\n",
    "# позволяет снизить скорость динамического обучения на основе некоторых проверочных измерений.\n",
    "sheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# зададим состояние обучения\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "# отображение эпох\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs, \n",
    "                          position=0)\n",
    "\n",
    "# для обучающей части\n",
    "dataset.set_split('train')\n",
    "\n",
    "# отображение обучающей части\n",
    "train_bar = tqdm_notebook(desc='split=train', \n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "# для валидационной части\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val', \n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        \"\"\"\n",
    "         # Итерации по набору обучающих данных\n",
    "\n",
    "         # настройка: пакетный генератор, установить потери и метрику на 0, включить режим обучения\n",
    "        \"\"\"\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        \n",
    "        # указываем, что параметры модели могут изменяться\n",
    "        classifier.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # Тренеровочная программа состоит из 5 шагов\n",
    "            \n",
    "            # Шаг 1. Обнуление градиентов\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Шаг 2. Вычесляем вывод (предсказание)\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            \n",
    "            # Шаг 3. Вычисляем потери\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            \n",
    "            # Шаг 4. Используем потери для создания градиентов\n",
    "            loss.backward()\n",
    "            \n",
    "            # Шаг 5. Используем оптимизатор, чтобы сделать шаг градиента\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            # Вычисляем accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # обновить столбик отображения\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "        \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        \n",
    "        # Проход в цикле по проверочному набору данных\n",
    "        # настройка: создаем генератор пакетов, устанавливаем значения\n",
    "        # переменных loss и acc равными 0, включаем режим проверки\n",
    "        \n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        \n",
    "        # указываем, что параметры модели неизменяемы и отключаем дропаут\n",
    "        classifier.eval()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            \n",
    "            # делаем предсказание\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            \n",
    "            # шаг 3. вычисляем потери\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            # вычисляем accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "            \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "        sheduler.step(train_state['val_loss'][-1])\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл обучения использует созданные в начале объекты для обновления параметров модели так, чтобы ее эффективность со временем росла. Точнее говоря, цикл обучения состоит из двух циклов: внутреннего цикла по мини-пакетам из набора данных и внешнего цикла, повторяющего внутренний нужное число раз. Во внутреннем цикле для каждого мини-пакета вычисляется функция потерь, а параметры модели обновляются с помощью оптимизатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код оценки эффективности модели на выделенном контрольном наборе почти ничем не отличается от цикла проверки в процедуре обучения из предыдущего примера с одним лишь небольшим нюансом: задан фрагмент 'test', а не 'val'. Различие между этими двумя фрагментами набора данных состоит в том, что контрольный набор следует использовать как можно меньше. При каждом запуске обученной модели на контрольном наборе, принятии на основании этого новых решений относительно модели (например, изменения размера слоев) и повторной оценки заново обученной модели на контрольном наборе модельные решения смещаются в сторону контрольных данных. Другими словами, при частом повторении этого процесса контрольные данные не смогут служить точной мерой, как действительно выделенные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем лучшие параметры Loss и accuracy для проверок на тестовой выборки.\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Потери на тесте: 0.178\n",
      "Accuracy на тесте: 93.56\n"
     ]
    }
   ],
   "source": [
    "print(\"Потери на тесте: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Accuracy на тесте: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один метод оценки эффективности модели — вывод на основе новых данных и анализ того, работает ли она. Добавим обработчик текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Предсказание рейтинга обзора\n",
    "    Аргументы:\n",
    "        review (str): текст обзора\n",
    "        classifier (ReviewClassifier): обученная модель\n",
    "        vectorizer (ReviewVectorizer): соответствующий векторизатор\n",
    "        decision_threshold (float): численная граница,\n",
    "        разделяющая различные классы рейтинга\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    review = preprocess_text(review)\n",
    "    \n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index=0\n",
    "    \n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book - > 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rusancovs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "test_review = 'this is a pretty awesome book'\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(f\"{test_review} - > {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Просмотр весов модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, последний способ выяснить, хорошо ли работает модель после окончания обучения, — просмотреть веса модели и принять решение, правильно ли они выглядят. В случае перцептрона и свернутого унитарного кодирования эта задача достаточно проста, поскольку каждому весу модели соответствует ровно одно слово из словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее часто встречающиеся слова в позитивных откликах:\n",
      "--------------------------------------\n",
      "exceeded\n",
      "pleasantly\n",
      "delicious\n",
      "excellent\n",
      "hooked\n",
      "incredible\n",
      "hesitate\n",
      "addicted\n",
      "deliciousness\n",
      "downside\n",
      "fantastic\n",
      "perfection\n",
      "disappoint\n",
      "amazing\n",
      "yummmm\n",
      "nexcellent\n",
      "divine\n",
      "delicioso\n",
      "yum\n",
      "awesome\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "Наиболее часто встречающиеся слова в негативных откликах:\n",
      "--------------------------------------\n",
      "poisoning\n",
      "mediocre\n",
      "worst\n",
      "slowest\n",
      "meh\n",
      "underwhelmed\n",
      "overrated\n",
      "tasteless\n",
      "rudest\n",
      "downhill\n",
      "bland\n",
      "horrible\n",
      "redeeming\n",
      "nwon\n",
      "flavorless\n",
      "inedible\n",
      "unacceptable\n",
      "terrible\n",
      "disappointing\n",
      "unimpressed\n"
     ]
    }
   ],
   "source": [
    "# сортируем веса\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# Топ 20 слов\n",
    "print(\"Наиболее часто встречающиеся слова в позитивных откликах:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# Топ 20 негативных слов\n",
    "print(\"Наиболее часто встречающиеся слова в негативных откликах:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
